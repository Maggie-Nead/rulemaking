---
title: "The Payday Loan Rule"
subtitle: "Comment text copied from the proposed rule or other comments" 
author: ""
output:
    # pdf_document:
    #   toc: true
    #   keep_tex: true
    html_document:
      highlight: zenburn
      toc: true
      toc_float: true
      code_folding: hide
editor_options: 
  chunk_output_type: console
---


```{r global.options, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, 
                      cache = FALSE, 
                      fig.width=8.5, 
                      split = T,
                      fig.align = 'center', 
                      fig.path='figs/',
                      warning=FALSE, 
                      message=FALSE)


library(tidyverse)
library(magrittr)
library(tidytext)
library(xml2)
library(knitr)
library(kableExtra)

library(ggplot2); theme_set(theme_bw())
  options(
    ggplot2.continuous.color = "viridis",
    ggplot2.continuous.fill = "viridis"
  )
  scale_color_discrete <- function(...)
    scale_color_viridis_d(..., direction = -1)
  scale_fill_discrete <- function(...)
    scale_fill_viridis_d(..., direction = -1)
  
  
kablebox <- . %>%  knitr::kable() %>% 
  kable_styling() %>% 
  scroll_box(height = "400px")
```


This document covers how to collect, store, and summarize word-level (token-level) information about the relationship among text. The general approach applies to any word or phrase-level(token-level) attributes (topic, relative frequency, citations, plagarism), here I focus on a simple form of plagarism dection using a matching 10-word phrases.

Just as we can use repeated 10-word phrases to identify change between draft and final rules (see the Volker Rule example), we can use a 10-word (10-gram) moving window to identify which words in a comment are part of a 10-word phrase that also appears in the  other comments or the proposed rule.

Building on Wilkerson et al 2017, I first used this method to detect similarity and change in agency buget justifications and congressional approprations texts (see my 2017 polmeth poster and APSA paper) and then adapted it to identify coalitions and form letters with repeated text in public comments (SPSA paper here).

Below, I walk through R functions to parse and summarize repeated text, applying them to CFPB's Payday Loan rule.

The result is information about each word in each comment. Was this word part of a 10-word phrase that also appeared in the NPRM? Was this word part of a 10-word phrase that also appeared in other comments? If so, which ones? What percent of the text of each comment matches each other comment? Computation and data storage are trivial for a few comments, but expands exponentially, aproximarly the square of the number of words in all comments. I use CFPB's Payday Loan Rule to illustrate methods to triage input data for the rule by far the most public comments.

### Methods

I filter out most mass comments (I've every improving my methods for detecting mass comments) and focus on comments that came as attachments from identified organizations. I extract the text from these attachments and elimate files less than 10 KB (the size of a short paragraph, often a failed OCR). These raw text files are available [here](https://ssc.wisc.edu/~judgelord/comment_text/CFPB-2016-0025/).

The `comment_tengrams` function requires two inputs:
1. file paths to a set of comments 
2. a link to the NPRM text (in order to identify NPRM text repeated in comments)

This function also relies of a few custom helper functions to [parse rule text](https://github.com/judgelord/rulemaking/blob/master/functions/xml_rule_text.R), [clean text](https://github.com/judgelord/rulemaking/blob/master/functions/clean_string.R), and [parse and match ngrams](https://github.com/judgelord/rulemaking/blob/master/functions/tengram.R). 


```{r}
# load required functions from https://github.com/judgelord/rulemaking/blob/master/functions
source(here::here("functions", "xml_rule_text.R"))
source(here::here("functions", "clean_string.R"))
source(here::here("functions", "tengram.R"))

# a function to parse comments into 10-word phrases and identify matching phrases in other comments of the NPRM
comment_tengrams <- function(nprm, comments){
  
  # read in rule text from federal register
  pr_text <- xml_rule_text(nprm) %>% 
    summarise(text = text %>% clean_string() ) %>%
    unnest_tokens(tengram, text, token = "ngrams", n = 10)  %>% 
    filter(!is.na(tengram))
  
  # filter to file paths ending in txt
  d <- comments %>% 
    filter( str_detect(path, "txt")) %>% 
    # in SQL, CFPB file names are regs_dot_gov_document_id, shortened to document_id for now
    mutate( document_id = path %>% 
              str_remove(".*/")  %>% 
              str_remove("\\..*") 
    )
  
  # parse each document with the read_grams function 
  d %<>% 
    mutate(tengrams = path %>% map(possibly(read_grams, 
                                            otherwise = list(tengram = "404error")
                                            )
                                   ) 
           )
  
  # map each document to all others
  d %<>% 
    mutate(
      text = tengrams %>% 
        # diff with the nprm
        map2( list(pr_text$tengram), match_tibble) %>% 
        map(~rename(., nprm_phrase = match) ) %>% 
        # reassemble text from the first word of each ngram
        map(~mutate(., word = str_extract(ngram, "\\w+") ) ) %>% 
        # drop ngrams to save space 
        map(~select(., -ngram) ),
      # diff with all other comments 
      reuse = tibble(document_id2 = list(document_id),
                     reuse = tengrams %>% map(~map2(., d$tengrams, match))
                     ) 
      ) %>% 
    # turn the tibble of lists into a list of tibbles
    group_by(document_id) %>% 
    mutate(reuse = reuse %>% purrr::flatten() %>%  as_tibble() %>% list() ) %>% 
    ungroup()
  
  # drop variables that we no longer need
  d %<>%  select(-path, -tengrams)
  
  return(d)
} # end function
```

### Data

```{r}
agency <- "CFPB"
docket <- "CFPB-2016-0025"
nprm <- "https://www.federalregister.gov/documents/full_text/xml/2016/07/22/2016-13490.xml"

# get txt files from a directory with here() 
comments <- tibble( path = list.files(here::here("comment_text", 
                                                 agency, 
                                                 docket),
                                      full.names = T )) 

# apply function
d <- comment_tengrams(nprm, 
                      comments %>% top_n(50) #FIXME just a few comments for now 
                      )
```

### Output
```{r}
d
```

Each `document_id` now has two associated data frames:
- `text` contains two columns:
  - `word` is the document's first 10,000 words
  - `nprm_phrase` is whether the that word is part of a ten-word phrase that is in the NPRM
- `reuse` contains a data frame for each other comment 
  - `document_id2`
  - `reuse` indicates whether each word in the document,`document_id`, is part of a ten-word phrase that appears in the second document, `document_id2`.

```{r nprm_percent_match}
d$text[1:2]


```



```{r comment_percent_match}
d$reuse[1:2]

comment_percent_match <- d %>% 
  top_n(20, document_id) %>% 
  select(-text) %>% # select just the doc name and reuse table (otherwise unnest duplicates the text table for every reuse observation)
  unnest(reuse) %>%             # unnest reuse tibble
  unnest(document_id2, reuse) %>% # unnest document_id and reuse lists 
  unnest(reuse) %>% # unnest reuse logical
  group_by(document_id, document_id2) %>% 
  summarise(percent_match = sum(reuse)/n() ) 

comment_percent_match %>% filter(document_id = "CFPB-2016-0025-95976") # select a file
```

```{r comment_percent_match_plot}
comment_percent_match %>% 
  mutate(document_id = document_id %>% str_remove(".*0025-"), 
         document_id2 = document_id2 %>% str_remove(".*0025-") ) %>% 
  ggplot() +
  aes(x = document_id, 
      y = document_id2, 
      fill = percent_match) + 
  geom_tile(color = "grey") + 
  scale_fill_gradient(low = "white", high = "black") + 
  theme(panel.grid = element_blank(),
        panel.border = element_blank(),
        axis.text.x = element_text(angle = 45))
  
```

Notice that 100% of the words from CFPB-2016-0025-95976 ([pdf](https://www.regulations.gov/document?D=CFPB-2016-0025-95976), [txt](https://ssc.wisc.edu/~judgelord/comment_text/CFPB-2016-0025/CFPB-2016-0025-95976-1.txt)
) are part of a tengram that is also in CFPB-2016-0025-95977 ([pdf](https://www.regulations.gov/document?D=CFPB-2016-0025-95977), [txt](https://ssc.wisc.edu/~judgelord/comment_text/CFPB-2016-0025/CFPB-2016-0025-95977-1.txt)) because the exact same comment was uploaded twice. 


```{r extra, include=FALSE, eval = FALSE}

################################################################################
# tests 

# copied from the NPRM
unlist(d$text[[3]]$word)[unlist(d$text[[3]]$match)] %>% paste(collapse =" ")
#FIXME with purrr


d$reuse
# reuse with itself should be all TRUE
# reuse for document 3
d$reuse[[3]]$reuse[[1]][[3]]



d$text
d %>% filter(nrow(text) > 1)


```

