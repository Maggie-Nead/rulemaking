---
title: "CFPB Payday Loan Rule"
subtitle: "Comments summarized with `textrank` compared to hand-coded key sentences"
output:
    # pdf_document:
    #   toc: true
    #   keep_tex: true
    html_document:
      highlight: zenburn
      #toc: true
      # toc_float: true
      code_folding: hide
editor_options: 
  chunk_output_type: console
---


```{r global.options, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, 
                      cache = TRUE, 
                      fig.width=8.5, 
                      split = T,
                      fig.align = 'center', 
                      fig.path='figs/',
                      warning=FALSE, 
                      message=FALSE)


library(tidyverse)
library(magrittr)
library(tidytext)
library(xml2)
library(knitr)
library(kableExtra)

library(ggplot2); theme_set(theme_bw())
  options(
    ggplot2.continuous.color = "viridis",
    ggplot2.continuous.fill = "viridis"
  )
  scale_color_discrete <- function(...)
    scale_color_viridis_d(..., direction = -1)
  scale_fill_discrete <- function(...)
    scale_fill_viridis_d(..., direction = -1)
  
  
kablebox <- . %>%  knitr::kable() %>% 
  kable_styling() %>% 
  scroll_box(height = "400px")
```

`summarize_comment` is a wrapper for `textrank_sentences` that returns a summary for each comment. It takes three inputs:

1. Comment `text` 
1. The number of sentences to return, `n_sentences` (optional)--this controls the length of the summary. Do you want each comment summarized in one sentence summary or five? The current default is two sentences per comment, but I plan to make this vary by a comment's length (longer comment may deserve a longer summary).  
1. The maximum number of sentences per comment to summarize `max_sentences` (optional). Fewer sentences makes textrank go faster, but we will often want to use all sentences. The default is 100 sentences per comment.

When the function is done, I'll add it to the textrank.R file, available [here](https://github.com/judgelord/rulemaking/blob/master/functions/textrank.R). Load it in R with `source("https://raw.githubusercontent.com/judgelord/rulemaking/master/functions/textrank.R")`

```{r}
# load summarize_comment() function
source("https://raw.githubusercontent.com/judgelord/rulemaking/master/functions/textrank.R")

# load text cleaning function 
source(here::here("functions", "clean_string.R"))
```


To assess the quality of computer-generated summaries, we can compare them to the key sentences identified by hand. Specifically, RAs were asked to identify the top three key "asks" in each comment and select the text associated with those askes in each comment. They also identified the commener's organization, overall position on the proposed rule, and several other key things that might help assess the quality of textrank-generated summaries. 

Load hand-coded data.

```{r gs}
load(here::here("data", "CFPB-2016-0025-coded.Rdata"))

coded %<>% mutate(hand_selected_summary = paste(ask, ask1, ask2, ask3, sep = "...") %>% str_remove_all("\\...NA")) 
```


Get file paths comment texts.

```{r data}
# file paths for comments 
agency <- "CFPB"
docket <- "CFPB-2016-0025"
# get txt file names from a directory, here called “comment_text”
comments <- tibble( path = list.files( here::here('comment_text', agency, docket), 
                                       full.names = T) ) %>% 
  filter( str_detect(path, "txt") )

# in SQL, CFPB file names are regs_dot_gov_document_id, shortened to document_id for now
comments %<>% 
  mutate( document_id = path %>%
            str_remove(".*/")  %>%
            str_remove("-[0-9]*\\..*") 
   ) 


d <- comments %>% 
  # select comments that have been hand coded
  filter(document_id %in% coded$document_id)

d %<>% head() #FIXME just working with a few comments for now
```

Clean comment texts.

```{r summarize}
# remove some common things. This can be generalized with info from data
# comment_header <- c("Docket No. CFP Consumer Financial Protection Bureau G Street, NW. Washington, DC")
# 
# comment_footer <- c("Sincerely,|Signature|Print Name|Address|City|State|Docket No\\. CFP|Zip|Email|Date")
# 
# agency_address <- c("Dear Consumer Financial Protection Bureau CFPB")

clean_string <- . %>% 
  str_c(collapse = " ") %>% 
  # make sure sentences are ended
  str_replace_all("\\t|\\\n", " ") %>% 
  str_remove_all("<|>") %>%
  str_remove_all("[0-9]\\..*\\.") %>%
  #str_replace_all("\\.([A-z])", ". \\1") %>%
  # remvove numbers and specials (keep only text and basic punctuation)
  str_remove_all("\\\\") %>% 
  str_replace_all("[^[A-z] \\.\\,\\?\\!\\;&\\;<>]", " ") %>% 
  #str_remove_all("\\'") %>%
  str_squish() %>%
  str_replace_all(" (\\.|\\?|\\!||:|;)", "\\1 ") %>%
  str_replace_all(" , ", " ") %>% 
  # double commas  
  #str_replace_all("\\, \\, ", ", ") %>% 
  # double periods 
  #str_replace_all("\\. \\. ", ". ") %>% 
  # one character after a period 
  str_replace_all("\\. .\\. ", ". ") %>% 
  # remove white space
  str_squish() %>% 
  #str_remove_all(comment_header) %>%
  #str_remove_all("pagebreak|<pagebreak>") %>% # Devin's OCR method adds this 
  #str_remove_all(comment_footer) %>%
  #str_remove_all(agency_address) %>%
  #str_replace_all("\\.*", " ") %>% 
  #str_remove_all("www.magnifymoney.com|magnifymoney") %>%
  # str_replace_all(" \\,", ", ") %>%
  str_replace_all(" \\.", ". ") %>%
  str_squish() 
```


```{r testing, include = FALSE, eval = FALSE}
read_comment <- . %>%
  read_lines() %>%
  clean_string() 

# map read
d$text <- map_chr(d$path, read_comment)

# inspect
d

d$text %>% kablebox()
```


Summarize comments.

```{r}
# # for testing 
# data <- d
# document_ids = d$document_id[1]
# max_sentences = 100


agency_text <- c("cfpb|CFPB|Consumer Financial Protection Bureau")

summarizeText <- function(document_ids, data, max_sentences) {
  
  # select document(s) provided in function input
  data %<>% filter(document_id == document_ids) 
  
  # load and clean text
  data$text <-  data$path %>% read_lines() %>% clean_string()
  
  sentences <- data %>%
    select(text) %>%
    unnest_sentences(output = sentences, input = text) %>%
    distinct() %>%
    mutate(textrank_id = row_number()) %>% 
    # textrank requires columns in order
    select(textrank_id, sentences)
  
  # select max sentences to summarize per section
  sentences %<>% filter(textrank_id <= max_sentences)
  
  
  sentences %>%
    anti_join(agency_text, copy = TRUE, by = character())
  
  # textrank needs a dictionary of words
  words <- unnest_tokens(sentences, output = word, input = 'sentences') %>% 
    distinct() %>% 
    anti_join(tidytext::stop_words)
  
  
  
  # textRank fails if you feed it only one sentence
  if(nrow(sentences) > 1){
    out <- textrank::textrank_sentences(data = sentences,
                                        terminology = words)
    
    # arrange by textrank 
    out$sentences %<>% arrange(-textrank)
    
    # so we format the unranked sentences data frame as an alternative
  } else {
    out <- list(sentences = as.data.frame(sentences))
  }
  
  #out %>%   knitr::kable() 
  
  return(out)
  
}


summarize_comments <- function(data, n_sentences = 2, max_sentences = 100) {
  

  # not cleaning data here either; should we? I think it is better to do it in the above function so it is done on one comment at a time.
  # but we can (must) create document_id here if we want to feed in data with just path (not relying on text or doc names)
  data %<>%  mutate( document_id = path %>%
            str_remove(".*/")  %>%
            str_remove("-[0-9]*\\..*") 
   ) 

  # summarize, map summary function to each unique section
  text_summary <-  map(.x = data$document_id %>% unique() ,
                       .f = summarizeText, 
                       data = data, 
                       max_sentences = max_sentences) 
  
  # extract top n_sentences
  pull_sentences <- . %>% 
    .$sentences %>% 
    pull(sentence) %>% 
    .[1:n_sentences] %>% 
    str_to_sentence() %>%  
    str_c(collapse = " ")
  
  # add n_sentences pulled from textrank output to data
  data$summary <- text_summary %>%
      #FIXME when this fails, it should default to the first sentence of the section. 
      map_chr(possibly(pull_sentences, otherwise = " ") ) 
  
  return(data)
}



comment_summary <- summarize_comments(d, n_sentences = 3, max_sentences = 50 )
```

Compare to hand-coded key sentences.

```{r}
comment_summary %>% 
  rename(textrank_summary = summary) %>%
  # join with coded data 
  left_join(coded) %>% 
  select(org_name, textrank_summary, hand_selected_summary, comment_url)  %>% kablebox()
```













