---
title: "Rules summarized with `textrank`"
subtitle: 
author: ""
output:
    # pdf_document:
    #   toc: true
    #   keep_tex: true
    html_document:
      highlight: zenburn
      toc: true
      toc_float: true
      code_folding: hide
editor_options: 
  chunk_output_type: console
---


```{r global.options, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, 
                      cache = TRUE, 
                      fig.width=8.5, 
                      split = T,
                      fig.align = 'center', 
                      fig.path='figs/',
                      warning=FALSE, 
                      message=FALSE)


library(tidyverse)
library(magrittr)
library(tidytext)
library(xml2)
library(knitr)
library(kableExtra)

library(ggplot2); theme_set(theme_bw())
  options(
    ggplot2.continuous.color = "viridis",
    ggplot2.continuous.fill = "viridis"
  )
  scale_color_discrete <- function(...)
    scale_color_viridis_d(..., direction = -1)
  scale_fill_discrete <- function(...)
    scale_fill_viridis_d(..., direction = -1)
  
  
kablebox <- . %>%  knitr::kable() %>% 
  kable_styling() %>% 
  scroll_box(height = "400px")
```

`summarize_comment` is a wrapper for `textrank_sentences` that returns a summary for each comment. It takes three inputs:

1. Comment `text` 
1. The number of sentences to return, `n_sentences` (optional)--this controls the length of the summary. Do you want each comment summarized in one sentence summary or five? The current default is two sentences per comment, but I plan to make this vary by a comment's length (longer comment may deserve a longer summary).  
1. The maximum number of sentences per comment to summarize `max_sentences` (optional). Fewer sentences makes textrank go faster, but we will often want to use all sentences. The default is 100 sentences per comment.

When the function is done, I'll add it to the textrank.R file.

You can download it [here](https://github.com/judgelord/rulemaking/blob/master/functions/textrank.R) or load it in R with `source("https://raw.githubusercontent.com/judgelord/rulemaking/master/functions/textrank.R")`


```{r}
# load summarize_comment() function
source("https://raw.githubusercontent.com/judgelord/rulemaking/master/functions/textrank.R")

# load text cleaning function 
source(here::here("functions", "clean_string.R"))
```

```{r data}
# file paths for comments 
agency <- "CFPB"
docket <- "CFPB-2016-0025"
# get txt file names from a directory, here called “comment_text”
comments <- tibble( path = list.files( here::here('comment_text', agency, docket), 
                                       full.names = T) )

# filter to file paths ending in txt
d <- comments %>%
  filter( str_detect(path, "txt")) %>%
  # in SQL, CFPB file names are regs_dot_gov_document_id, shortened to document_id for now
  mutate( document_id = path %>%
            str_remove(".*/")  %>%
            str_remove("\\..*") 
   ) 

d %<>% head()

read_comment <- . %>%
  read_lines() %>%
  clean_string() 

# map read
d$text <- map_chr(d$path, read_comment)

# inspect
d

```

# CFPB Payday Loan Rule

```{r summarize}

# clean_string <- . %>% 
#   str_c(collapse = " ") %>% 
#   # make sure sentences are ended
#   str_replace_all("\\t|\\\n", " ") %>%
#   #str_replace_all("\\s+"," ") %>%
#   #str_remove_all("\\(.*\\)|[0-9]|_") %>%
#   #str_replace_all("[^([A-z]& &'&\\.&\\,&\\?&\\!&\\;&\\;)]", " ") %>% 
#   #str_replace_all("\\.([A-z])", ". \\1") %>%
#   # remvove numbers and specials (keep only text and basic punctuation)
#   str_remove_all("\\\\") %>% 
#   str_replace_all("[^[A-z] \\.\\,\\?\\!\\;&\\;<>]", " ") %>% 
#   str_squish() %>%
#   str_replace_all(" (\\.|\\?|\\!||:|;)", "\\1 ") %>%
#   str_replace_all(" , ", " ") %>% 
#   # double commas  
#   #str_replace_all("\\, \\, ", ", ") %>% 
#   # double periods 
#   #str_replace_all("\\. \\. ", ". ") %>% 
#   # one character after a period 
#   str_replace_all("\\. .\\. ", ". ") %>% 
#   # remove white space
#   str_squish() %>% 
#   # str_replace_all(" \\,", ", ") %>%
#   str_replace_all(" \\.", ". ") %>%
#   str_squish() 


# clean_comment <- . %>%  
#   # drop short texts
#   #filter(nchar(d) > 60) %>% 
#   # identify headings as lines with a period in the beginning (also captures some footnotes)
#   #mutate(part = ifelse(str_detect(str_sub(d, 1, 5) ,'\\.'), 'head', 'd')) %>% 
#   # idetify footnotes with a number and no period
#   #mutate(part = ifelse(d %>% str_detect("^[0-9](?!\\.)"), 'footnote', part) ) %>% 
#   summarize(text = text %>% 
#            str_c(collapse = " ") %>% 
#            # CAUTION: removes all text in parentheses
#            str_replace_all("\\s+"," ") %>%
#            #str_remove_all("\\(.*\\)|[0-9]|_") %>%
#            #str_remove_all(d, "§") %>% 
#            str_squish() %>% 
#            # add space after periods
#            str_replace_all("\\.([A-Z][a-z])", ". \\1") %>%
#           # remvove numbers and specials (keep only text and basic punctuation)
#           str_replace_all("[^([A-z]& &'&\\.&\\,&\\?&\\!&\\;&\\;)]", " ") %>% 
#           # remove space before periods
#           str_replace_all(" \\.", ". ") %>%
#           # double commas  
#           str_replace_all("(\\, \\,) ", ", ") %>% 
#           # double periods 
#           str_replace_all("(\\. \\.|\\.\\.) ", ". ") %>% 
#            str_squish() %>%
#           # one character after a period 
#           str_replace_all("\\. .\\. ", ". ") %>% 
#           # remove white space
#           str_replace_all(" \\,", ", ") %>%
#           str_replace_all(" \\.", ". ") %>%
#            str_remove_all("pagebreak|http www\\.magnifymoney\\.com|docket no") %>%
#           str_squish() # %>% clean_string() # optional
#           ) #%>%
#   # filter(d != "")  #filter out blank strings
# 
# #d %<>% group_by(document_id) %>% clean_comment()
# 

# for testing 
data <- d
document_ids = d$document_id[1]
max_sentences = 100

summarizeText <- function(document_ids, data, max_sentences) {
  
  # not cleaning data here; should we?
  
  sentences <- data %>%
    # slect section provided in function input
    filter(document_id == document_ids) %>%
    select(text) %>%
    unnest_sentences(output = sentences, input = text) %>%
    distinct() %>%
    mutate(textrank_id = row_number()) %>% 
    # textrank requires columns in order
    select(textrank_id, sentences)
  
  # select max sentences to summarize per section
  sentences %<>% filter(textrank_id <= max_sentences)
  
  # textrank needs a dictionary of words
  words <- unnest_tokens(sentences, output = word, input = 'sentences') %>% 
    distinct() %>% 
    anti_join(tidytext::stop_words)
  
  # textRank fails if you feed it only one sentence
  if(nrow(sentences) > 1){
    out <- textrank::textrank_sentences(data = sentences,
                                        terminology = words)
    
    # arrange by textrank 
    out$sentences %<>% arrange(-textrank)
    
    # so we format the unranked sentences data frame as an alternative
  } else {
    out <- list(sentences = as.data.frame(sentences))
  }
  
  #out %>%   knitr::kable() 
  
  return(out)
  
}


summarize_comments <- function(data, n_sentences = 2, max_sentences = 100) {
  

  # not cleaning data here either; should we? I think it is better to do it in the above function so it is done on one comment at a time.

  # summarize, map summary function to each unique section
  text_summary <-  map(.x = data$document_id %>% unique() ,
                       .f = summarizeText, 
                       data = data, 
                       max_sentences = max_sentences) 
  
  # extract top n_sentences
  pull_sentences <- . %>% 
    .$sentences %>% 
    pull(sentence) %>% 
    .[1:n_sentences] %>% 
    str_to_sentence() %>%  
    str_c(collapse = " ")
  
  # add n_sentences pulled from textrank output to data
  data$summary <- text_summary %>%
      #FIXME when this fails, it should default to the first sentence of the section. 
      map_chr(possibly(pull_sentences, otherwise = " ") ) 
  
  return(data)
}



comment_summary <- summarize_comments(d, n_sentences = 2, max_sentences = 50 )

comment_summary %>% select(document_id, summary) # %>% kablebox()
```

