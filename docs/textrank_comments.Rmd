---
title: "CFPB Payday Loan Rule"
subtitle: "Comments summarized with `textrank` compared to hand-coded key sentences"
output:
    # pdf_document:
    #   toc: true
    #   keep_tex: true
    html_document:
      highlight: zenburn
      #toc: true
      # toc_float: true
      code_folding: hide
editor_options: 
  chunk_output_type: console
---


```{r global.options, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, 
                      cache = FALSE, 
                      fig.width=8.5, 
                      split = T,
                      fig.align = 'center', 
                      fig.path='figs/',
                      warning=FALSE, 
                      message=FALSE)


library(tidyverse)
library(magrittr)
library(tidytext)
library(xml2)
library(knitr)
library(kableExtra)

library(ggplot2); theme_set(theme_bw())
  options(
    ggplot2.continuous.color = "viridis",
    ggplot2.continuous.fill = "viridis"
  )
  scale_color_discrete <- function(...)
    scale_color_viridis_d(..., direction = -1)
  scale_fill_discrete <- function(...)
    scale_fill_viridis_d(..., direction = -1)
  
  
kablebox <- . %>%  knitr::kable() %>% 
  kable_styling() %>% 
  scroll_box(height = "500px")
```

`summarize_comment` is a wrapper for `textrank_sentences` that returns a summary for each comment. It takes three inputs:

1. Comment `text` 
1. The number of sentences to return, `n_sentences` (optional)--this controls the length of the summary. Do you want each comment summarized in one sentence summary or five? The current default is two sentences per comment, but I plan to make this vary by a comment's length (longer comment may deserve a longer summary).  
1. The maximum number of sentences per comment to summarize `max_sentences` (optional). Fewer sentences makes textrank go faster, but we will often want to use all sentences. The default is 100 sentences per comment.

When the function is done, I'll add it to the textrank.R file, available [here](https://github.com/judgelord/rulemaking/blob/master/functions/textrank.R). Load it in R with `source("https://raw.githubusercontent.com/judgelord/rulemaking/master/functions/textrank.R")`

```{r}
# load summarize_comment() function
source("https://raw.githubusercontent.com/judgelord/rulemaking/master/functions/textrank.R")

# load text cleaning function 
source(here::here("functions", "clean_string.R"))
```


To assess the quality of computer-generated summaries, we can compare them to the key sentences identified by hand. Specifically, RAs were asked to identify the top three key "asks" in each comment and select the text associated with those askes in each comment. They also identified the commener's organization, overall position on the proposed rule, and several other key things that might help assess the quality of textrank-generated summaries. 

Load hand-coded data.

```{r gs}
load(here::here("data", "CFPB-2016-0025-coded.Rdata"))

coded %<>% mutate(hand_selected_summary = paste(ask, ask1, ask2, ask3, sep = "...") %>% str_remove_all("\\...NA")) 
```


Get file paths comment texts.

```{r data}
# file paths for comments 
agency <- "CFPB"
docket <- "CFPB-2016-0025"

# A dataframe of custom stop words, formatted like tidytext::stop_words
# If form letters need to be summariezed standard headings should be removed (docket no., email, etc.)
custom_stop_words <- tibble( 
  words = c("cfpb", "payday, vehicle title, and certain high-cost installment loans")) %>% # ignore case?

  mutate(words = words)%>%
  unnest_tokens(output = word, input = words) %>%
  distinct()

#agency_name <- c("consumer financial protection bureau")

# get txt file names from a directory, here called “comment_text”
comments <- tibble( path = list.files( here::here('comment_text', agency, docket), 
                                       full.names = T) ) %>% 
  filter( str_detect(path, "txt") )

# in SQL, CFPB file names are regs_dot_gov_document_id, shortened to document_id for now
comments %<>% 
  mutate( document_id = path %>%
            str_remove(".*/")  %>%
            str_remove("-[0-9]*\\..*") 
   ) 


d <- comments %>% 
  # select comments that have been hand coded
  filter(document_id %in% coded$document_id)

#d %<>% head() #FIXME just working with a few comments for now
```

Clean comment texts.

```{r summarize}
# remove some common things. This can be generalized with info from data

# These should go in custom_stop_words, not here; this is valid text.
# Only needed if we are capturing summaries for form letters
# comment_header <- c("Docket No. CFP Consumer Financial Protection Bureau G Street, NW. Washington, DC")
# 
# comment_footer <- c("Sincerely,|Signature|Print Name|Address|City|State|Docket No\\. CFP|Zip|Email|Date")
# 
# agency_address <- c("Dear Consumer Financial Protection Bureau CFPB")

clean_comments <- . %>% 
  str_c(collapse = " ") %>% 
  # remove tab and line breaks
  str_replace_all("\\t|\\\n", " ") %>% 
  str_replace_all("U\\.S\\.", "United States") %>% #Issues with cut of sentences due to period
  #str_remove_all(" ?(f|ht)(tp)(s?)(://)([A-z]*)[.|/](.*)") %>%
  #str_replace_all("\\s+", " ") %>% # removed by str_squish()
  #str_remove_all("^[0-9](?!\\.)") %>% #FIXME are not numbers removed below
  # add missing spaces after periods
  str_replace_all("\\.([A-z])", ". \\1") %>%
  # remvove numbers and specials (keep only text and basic punctuation)
  str_replace_all("[^[A-z] \\.\\,\\?\\!\\;&\\;\\']", " ") %>% 
  str_remove_all("\\[|\\]") %>%
  str_remove_all("§") %>%
  #str_remove_all("\\'") %>%
  str_squish() %>%
  #str_replace_all(" (\\.|\\?|\\!||:|;)", "\\1 ") %>%
  str_replace_all(" , ", " ") %>%
  str_replace_all("_", " ") %>%
  # double commas  
  #str_replace_all("\\, \\, ", ", ") %>% 
  # double periods 
  #str_replace_all("\\. \\. ", ". ") %>% 
  # one character after a period 
  str_replace_all("\\. \\. \\. ", ". ") %>% 
  # remove white space
  str_squish() %>% 
  str_remove_all("pagebreak") %>% # Devin's OCR method adds this 
  # remove repeated periods
  #str_replace_all("\\.*", ". ") %>% #removed below
  # str_replace_all(" \\,", ", ") %>%
  str_replace_all(" \\.", ". ") %>%
  #remove space in 's
  str_replace_all(" \\'s ", "\\'s ") %>%
  #str_replace_all(" '", "'") %>% # we want to keep "'"
  # remove web addresses, wont capture urls with punctuation ("." or "_") in the middle
  # str_remove_all("www\\.[A-z]*\\.(com|org|net|gov|pdf)") %>%  # does not currently impact textrank, uncomment if urls start to skew summary
  # str_remove_all("http:www\\.[A-z]*\\.(com|org|net|gov|pdf)") %>%
  # str_remove_all("files\\.[A-z]*\\.(com|org|net|gov|pdf)") %>%
  #Removes duplicated puncuation
  str_replace_all("([[:punct:]])\\1+", "\\1") %>%
  str_squish() 

read_comments <- . %>%
  read_lines() %>%
  clean_comments() 

# map read
d$text <- map_chr(d$path, read_comment)
```


```{r testing, include = FALSE, eval = FALSE}
# inspect
d$text[10]

d$text %>% kablebox()

#Missing files
coded %>%
  filter(document_id %in% coded$document_id) %>%
  filter(document_id %in% d$document_id == FALSE) %>%
  select(document_id, comment_url) %>%
  kablebox()
  
```


Summarize comments.

```{r}
# for testing 
# text <- d$text[4]
# max_sentences = 100

summarizeText <- function(text, max_sentences, custom_stop_words, agency_name) {

  sentences <- tibble(text = text) %>%
    unnest_sentences(output = sentences, input = text) %>%
    distinct() %>%
    mutate(textrank_id = row_number()) %>% 
    # textrank requires columns in order
    select(textrank_id, sentences)
  
  # remove agency name string from textrank dictionary
  sentences %<>%
    mutate(sentences = str_remove_all(sentences, agency_name))
   
  # select max sentences to summarize per section
  sentences %<>% filter(textrank_id <= max_sentences)
  
  
  # textrank needs a dictionary of words
  words <- unnest_tokens(sentences, output = word, input = 'sentences') %>% 
    distinct() %>% 
    anti_join(tidytext::stop_words) %>% 
    anti_join(custom_stop_words) # remove custom stop words from textrank
  
  # inspect 
  count(words, word, sort = T) %>% filter(n>2) %>% pull(word)
     
  # textRank fails if you feed it only one sentence
  if(nrow(sentences) > 1){
    out <- textrank::textrank_sentences(data = sentences,
                                        terminology = words)
    
    
    # arrange by textrank 
    out$sentences %<>% arrange(-textrank)
    
    # so we format the unranked sentences data frame as an alternative
  } else {
    out <- list(sentences = as.data.frame(sentences))
  }
  
  #out %>%   knitr::kable() 
  
  return(out)
  
}


summarize_comments <- function(text, n_sentences = 2, max_sentences = 100, custom_stop_words, agency_name = "") {
  

  # not cleaning data here either; should we? I think it is better to do it in the above function so it is done on one comment at a time.

  # summarize, map summary function to each unique section
  text_summary <-  map(.x = text,
                       .f = summarizeText,
                       max_sentences = max_sentences,
                       custom_stop_words = custom_stop_words,
                       agency_name = agency_name) 
  
  # extract top n_sentences
  pull_sentences <- . %>% 
    .$sentences %>% 
    pull(sentence) %>% 
    .[1:n_sentences] %>% 
    str_to_sentence() %>%  #FIXME should define a custom function to fix sentence case with some common fixes like:
    str_replace_all(" i ", " I ") %>% 
    str_replace_all(" u\\.s\\. ", " U.S. ") %>% 
    str_replace_all("nprm", "NPRM") %>% 
    str_c(collapse = " ")
  
  # add n_sentences pulled from textrank output to data
  summary <- text_summary %>%
      #FIXME when this fails, it should default to the first sentence of the section. 
      map_chr(possibly(pull_sentences, otherwise = " ") ) 
  
  return(summary)
}



comment_summary <- summarize_comments(d$text, 
                                      n_sentences = 3, 
                                      max_sentences = 50, 
                                      custom_stop_words = custom_stop_words,
                                      agency_name = agency_name)
```



Compare to hand-coded key sentences.

```{r}
d %>% 
  mutate(textrank_summary = comment_summary) %>%
  # join with coded data 
  left_join(coded) %>% 
  # a table comparing textrank vs hand coded
  select(org_name, textrank_summary, hand_selected_summary, comment_url, comment_txt)  %>% kablebox()
```


Function removes agency name from Textrank. This doesn't impact every summary but the ones below are changed and generally the better summary is the one with the removed agency name. 


# Summary changes with and without agency name:

Typically this change swaps out one sentence in the summary. When agency name is removed this tends to run the risk of capturing headings. Each option has their pros and cons and each method produces some good summaries and some bad summaries.

## Examples:


## Organization: Idaho Financial Services Association Inc

With agency name:
Ifsa and its members to express serious concerns about the consumer financial protection bureau s cfpb recent proposed rule for payday, vehicle title, and certain high cost installment loans proposed rule. The net effect of including optional voluntary protection products in the proposed rule s trigger is that likely many of our members will no longer make voluntary protection products available to their borrowers. Including optional voluntary protection products as part of the proposed rule s trigger treats voluntary protection products as percent profit to the lender and of zero value to the consumer.

Without Agency:
The net effect of including optional voluntary protection products in the proposed rule s trigger is that likely many of our members will no longer make voluntary protection products available to their borrowers. Ifsa and its members to express serious concerns about the s cfpb recent proposed rule for payday, vehicle title, and certain high cost installment loans proposed rule. Including optional voluntary protection products as part of the proposed rule s trigger treats voluntary protection products as percent profit to the lender and of zero value to the consumer.


## Organization: The Chippewa Cree Tribe of the Rocky Boys Reservation

With Agency:
Once again, nowhere in the NPRM does the bureau acknowledge tribal regulation in the small dollar lending industry. There is no discussion whatsoever of tribal regulation of small dollar lending, an omission that undercuts any empirical foundation for the proposed rule and significantly and negatively impacts the rulemaking analysis. **Because of all these deficiencies, the bureau should rescind the proposed rule entirely.**

Without Agency:
Once again, nowhere in the NPRM does the bureau acknowledge tribal regulation in the small dollar lending industry. **Jackson on behalf of the chippewa cree tribe of the rocky boy s reservation tribe I submit this public comment to express the tribe s views of the proposed regulations from the cfpb or bureau regarding small dollar lending proposed rule.** There is no discussion whatsoever of tribal regulation of small dollar lending, an omission that undercuts any empirical foundation for the proposed rule and significantly and negatively impacts the rulemaking analysis.


## Organization: Ballard Spahr

With Agency: 
The bureau ' s own analysis shows that its proposed rule w ill have a devastating impact both on consumers who rely on covered loans and on providers of covered loans. **The proposed rule was published by the consumer financial protection bureau cfpb or ' bureau in fed.** I the proposed rule.

Without Agency:
The bureau ’ s own analysis shows that its proposed rule w ill have a devastating impact both on consumers who rely on covered loans and on providers of covered loans. I the proposed rule. **Monica jackson october page executive summary consumers rely upon loans subject to the proposed rule covered loans to pay for emergency expenses, to avoid late charges and nsf fees, and to add ress other serious financia l needs.**


## Organization: Katten Muchin Rosenman LLP

With Agency:
**Jackson the comments set forth below are provided in connection with the payday, vehicle title and certain high cost installment loans proposal set forth by the consumer financial protection bureau cfpb or bureau in the federal register on july the proposal.** Like many providers included within the scope of the proposal, our client appreciates this opportunity to comment on the first regulatory exercise of the cfpb's statutory authority to deter unfair and abusive acts and practices in connection with the provision of consumer financial products and services. Dfa o.


Without Agency:
Like many providers included within the scope of the proposal, our client appreciates this opportunity to comment on the first regulatory exercise of the cfpb’s statutory authority to deter unfair and abusive acts and practices in connection with the provision of consumer financial products and services. **See dfa l c l. Dfa I a.**




```{r investigate}
d %>% 
  mutate(textrank_summary = comment_summary) %>%
  # join with coded data 
  left_join(coded) %>% 
  # a table comparing textrank vs hand coded
  select(org_name, textrank_summary, hand_selected_summary, comment_url, comment_txt)  %>% 
  filter(org_name == "Ballard Spahr") %>%
  kablebox()


```

Textrank is capturing split words as important. Once this is fixed in the loading of text files this could change the summaries with more words being captured by textrank

```{r spliceSentences}
d %>% 
  mutate(textrank_summary = comment_summary) %>%
  # join with coded data 
  left_join(coded) %>% 
  # a table comparing textrank vs hand coded
  select(org_name, textrank_summary, hand_selected_summary, comment_url, comment_txt)  %>% 
  filter(org_name == "The Independent Finance Association of Illinois") %>%
  kablebox()
  
```


Textrank is capturing state names & Summary has output from footnote that is not captured by textrank
```{r stateNames}
d %>% 
  mutate(textrank_summary = comment_summary) %>%
  # join with coded data 
  left_join(coded) %>% 
  # a table comparing textrank vs hand coded
  select(org_name, textrank_summary, hand_selected_summary, comment_url, comment_txt)  %>% 
  filter(org_name == "State Attorney Generals") %>%
  kablebox()

```


# General Observations about Textrank

Textrank tends to capture specifics instead of firm stances. The coded comment summaries tend to explain why an organization is in support or in opposition to a rule but will not usually spell out whether or not the comment supports or does not support. Generated summaries more accurately capture changes an organization would like to see to a rule instead of attitudes about the rule as a whole. The hand coded summary contains the ask and the generated summary contains and explanation of that ask.

## Examples

```{r obs}
 d %>% 
  mutate(textrank_summary = comment_summary) %>%
  # join with coded data 
  left_join(coded) %>% 
  # a table comparing textrank vs hand coded
  select(org_name, textrank_summary, hand_selected_summary, comment_url, comment_txt)  %>% 
  filter(org_name == c("McIntyre & Lemon, PLLC", "The Independent Finance Association of Illinois", "Community Spirit Bank")) %>%
  kablebox()

```






