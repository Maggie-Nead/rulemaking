---
title: "Rules summarized with `textrank`"
subtitle: 
author: ""
output:
    # pdf_document:
    #   toc: true
    #   keep_tex: true
    html_document:
      highlight: zenburn
      toc: true
      toc_float: true
      code_folding: hide
editor_options: 
  chunk_output_type: console
---


```{r global.options, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, 
                      cache = TRUE, 
                      fig.width=8.5, 
                      split = T,
                      fig.align = 'center', 
                      fig.path='figs/',
                      warning=FALSE, 
                      message=FALSE)


library(tidyverse)
library(magrittr)
library(tidytext)
library(xml2)
library(knitr)
library(kableExtra)

library(ggplot2); theme_set(theme_bw())
  options(
    ggplot2.continuous.color = "viridis",
    ggplot2.continuous.fill = "viridis"
  )
  scale_color_discrete <- function(...)
    scale_color_viridis_d(..., direction = -1)
  scale_fill_discrete <- function(...)
    scale_fill_viridis_d(..., direction = -1)
  
  
kablebox <- . %>%  knitr::kable() %>% 
  kable_styling() %>% 
  scroll_box(height = "400px")
```

`summarize_comment` is a wrapper for `textrank_sentences` that returns a summary for each comment. It takes three inputs:

1. Comment `text` 
1. The number of sentences to return, `n_sentences` (optional)--this controls the length of the summary. Do you want each comment summarized in one sentence summary or five? The current default is two sentences per comment, but I plan to make this vary by a comment's length (longer comment may deserve a longer summary).  
1. The maximum number of sentences per comment to summarize `max_sentences` (optional). Fewer sentences makes textrank go faster, but we will often want to use all sentences. The default is 100 sentences per comment.

When the function is done, I'll add it to the textrank.R file.

You can download it [here](https://github.com/judgelord/rulemaking/blob/master/functions/textrank.R) or load it in R with `source("https://raw.githubusercontent.com/judgelord/rulemaking/master/functions/textrank.R")`


```{r}
# load summarize_comment() function
source("https://raw.githubusercontent.com/judgelord/rulemaking/master/functions/textrank.R")
```

```{r data}
agency <- "CFPB"
docket <- "CFPB-2016-0025"
nprm <- "https://www.federalregister.gov/documents/full_text/xml/2016/07/22/2016-13490.xml"
# get txt file names from a directory, here called “comment_text”
comments <- tibble( path = list.files( here::here('comment_text', agency, docket), 
                                       full.names = T) )

# filter to file paths ending in txt
d <- comments %>%
  filter( str_detect(path, "txt")) %>%
  # in SQL, CFPB file names are regs_dot_gov_document_id, shortened to document_id for now
  mutate( document_id = path %>%
            str_remove(".*/")  %>%
            str_remove("\\..*") 
   ) 

read_comment <- . %>%
  read_lines() %>%
  clean_string() 

d$text <- map_chr(d$path, read_comment)

d
```

# CFPB Payday Loan Rule

```{r summarize}


# clean_comment <- . %>%  
#   # drop short texts
#   #filter(nchar(d) > 60) %>% 
#   # identify headings as lines with a period in the beginning (also captures some footnotes)
#   #mutate(part = ifelse(str_detect(str_sub(d, 1, 5) ,'\\.'), 'head', 'd')) %>% 
#   # idetify footnotes with a number and no period
#   #mutate(part = ifelse(d %>% str_detect("^[0-9](?!\\.)"), 'footnote', part) ) %>% 
#   summarize(text = text %>% 
#            str_c(collapse = " ") %>% 
#            # CAUTION: removes all text in parentheses
#            str_replace_all("\\s+"," ") %>%
#            #str_remove_all("\\(.*\\)|[0-9]|_") %>%
#            #str_remove_all(d, "§") %>% 
#            str_squish() %>% 
#            # add space after periods
#            str_replace_all("\\.([A-Z][a-z])", ". \\1") %>%
#           # remvove numbers and specials (keep only text and basic punctuation)
#           str_replace_all("[^([A-z]& &'&\\.&\\,&\\?&\\!&\\;&\\;)]", " ") %>% 
#           # remove space before periods
#           str_replace_all(" \\.", ". ") %>%
#           # double commas  
#           str_replace_all("(\\, \\,) ", ", ") %>% 
#           # double periods 
#           str_replace_all("(\\. \\.|\\.\\.) ", ". ") %>% 
#            str_squish() %>%
#           # one character after a period 
#           str_replace_all("\\. .\\. ", ". ") %>% 
#           # remove white space
#           str_replace_all(" \\,", ", ") %>%
#           str_replace_all(" \\.", ". ") %>%
#            str_remove_all("pagebreak|http www\\.magnifymoney\\.com|docket no") %>%
#           str_squish() # %>% clean_string() # optional
#           ) #%>%
#   # filter(d != "")  #filter out blank strings
# 
# #d %<>% group_by(document_id) %>% clean_comment()
# 

  


summarizeText <- function(document_id, text, max_sentences) {
  
  sentences <- text %>%
    # slect section provided in function input
    filter(document_id == document_id) %>%
    select(text) %>%
    unnest_sentences(output = sentences, input = text) %>%
    distinct() %>%
    mutate(textrank_id = row_number()) %>% 
    # textrank requires columns in order
    select(textrank_id, sentences)
  
  # select max sentences to summarize per section
  sentences %<>% filter(textrank_id <= max_sentences)
  
  # textrank needs a dictionary of words
  words <- unnest_tokens(sentences, output = word, input = 'sentences') %>% 
    distinct() %>% 
    anti_join(tidytext::stop_words)
  
  # textRank fails if you feed it only one sentence
  if(nrow(sentences) > 1){
    out <- textrank::textrank_sentences(data = sentences,
                                        terminology = words)
    # so we format the unranked sentences data frame as an alternative
  } else {
    out <- list(sentences = as.data.frame(sentences))
  }
  
  return(out)
  
}


summarize_comments <- function(data, n_sentences = 2, max_sentences = 100) {
  
  # section and clean text 
  data %<>% group_by(document_id) #%>% clean_comment()  #%>% 
    # drop headers and footnotes
    #filter(!is.na(section), part == "text") %>%
    #FIXME make max_sentences depend on section length
    # group_by(section) %>% 
    #add_count(document_id, name = "document_length") 
  

  # summarize, map summary function to each unique section
  text_summary <-  map(.x = data$document_id %>% unique() ,
                       .f = summarizeText, 
                       text = data, 
                       max_sentences = max_sentences) 
  
  # extract top n_sentences
  pull_sentences <- . %>% 
    .$sentences %>% 
    pull(sentence) %>% 
    .[1:n_sentences] %>% 
    str_to_sentence() %>%  
    str_c(collapse = " ")
  
  # make a data frame of section headers and n_sentences pulled from textrank output
  comment_summary <- tibble(
    comment = pull(data, document_id) %>% unique(), 
    summary = text_summary %>%
      #FIXME when this fails, it should default to the first sentence of the section. 
      map_chr(possibly(pull_sentences, otherwise = " ") ) ) 
  
  return(comment_summary)
}

comment_summary <- summarize_comments(d, n_sentences = 2, max_sentences = 50 )

comment_summary %>%   knitr::kable()

comment_summary

# d %>%
#   select(text)
```

