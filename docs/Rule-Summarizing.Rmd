---
title: "Rule Summarizing"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

This overviews summarizing rules using the TextRank R package, which is an application of the Google PageRank algorithm to sentences. The paper behind the package is available [here](https://web.eecs.umich.edu/~mihalcea/papers/mihalcea.emnlp04.pdf). Keywords are extracted from each sentence and a weighted graph is built estimating connected sentences. As a proof of concept, I've applied it to a (relatively) straightforward case. The rule is "Margin and Capital Requirements for Covered Swap Entities," a multi-agency final rule from 2016, available (here)[https://www.federalregister.gov/documents/2016/08/02/2016-18193/margin-and-capital-requirements-for-covered-swap-entities]. 

After tweaking the inputs iteratively, it looks like this may be an effective tool to summarize rules. A suggestion would be to apply it over single documents (such as comments) to get quick results and over sections of very long rules. Running this on a rule like the Volker Rule without reasoned subsetting would be difficult, as it constructs a Jaccard index in approximately $O(n^2)$ time where n is the number of sentences. For this example, it takes approximately 30 seconds to run from download to summary on 179 sentences.

Below is an overview of the process:

First, load a basic set of libraries, nothing too extraordinary.
```{r libs, warning=FALSE, message=FALSE}
library(tidyverse)
library(textrank)
library(xml2)
library(tidytext)
```



Then, download the rule. This could equivalently be used by loading a comment from file, but with slightly different processing.
```{r Gather Data}
link <- 'https://www.federalregister.gov/documents/full_text/xml/2016/08/02/2016-18193.xml'

xml_path <- download_xml(url = link)
xml_parse <- read_xml(xml_path) %>% xml_children() %>% xml_children() %>% xml_text()
text <- str_split(xml_parse, pattern = '\n') %>% unlist() %>% str_squish()
```

Next, we need to process the text of the rule. Steps include:
1. Remove blank strings.
2. Identify if the first character is a number to avoid including footnotes.
3. Identify headings to avoid useless information (such as "I. Background") for computational purposes.
4. Identify the text before the actual rule to avoid the summary and people's names.
5. Remove all numbers, as they trick the algorithm too much.

```{r Prep Data}
suppressWarnings(
text_df <- tibble(text = text) %>% filter(text != "") %>% mutate(id = row_number()) %>% 
                   mutate(first_char = str_sub(text, 1, 1), part = 'text') %>%  rowwise() %>% 
                   mutate(earlydot = str_detect(str_sub(text, 1, 5) ,'\\.')) %>% 
                   mutate(part = ifelse(earlydot, 'head', part)) %>% 
                   mutate(first_char = as.numeric(first_char)) %>%
                   mutate(part = ifelse(is.na(first_char), part, 'footnote')) %>% 
                   mutate(part = ifelse(id < 26, 'pre', part)) %>% 
                   ungroup() %>% mutate(text = gsub('[[:digit:]]+', '', text))
)
```
              
Once we've cleaned the text, we need to extract the sentences and words, using tidytext.    
```{r tokenize}
text_clean <- text_df %>% filter(part == 'text') %>% select(text)
text_sentences <- unnest_sentences(text_clean, output = sentences, input = text)
text_sentences <- text_sentences %>% unique() %>% mutate(textrank_id = row_number()) 
text_sentences <- text_sentences[,c(2,1)]

data("stop_words")
word <- unnest_tokens(text_sentences, output = word, input = 'sentences') %>% 
  unique() %>% 
  filter(!word %in% stop_words$word)
```

This gives two tibbles.
```{r glance}
head(text_sentences)
head(word)
```

Now we can run the algorithm, which is fairly quick at this point.
```{r Run textrank}
out <- textrank::textrank_sentences(data = text_sentences, terminology = word)
```

We can finally extract a summary:
```{r,echo=FALSE}
summary(out, n = 5, keep.sentence.order =T)
```

[1] "qualifying non-cleared swaps and non-cleared security-based swaps of entities covered by section  of tripra are not subject to the agencies' joint final rule."                                                                                                                                                                                                                                                              
[2] "tripra provides that the initial and variation margin requirements of the joint final rule shall not apply to a non-cleared swap in which a counterparty qualifies for an exception under section (h)()(a) of the commodity exchange act or a non-cleared security-based swap in which a counterparty qualifies for an exception under section c(g)() of the securities exchange act."                                       
[3] "a counterparty that is not a financial entity  and that is using swaps to hedge or mitigate commercial risk generally would qualify for an exception from clearing under section (h)()(a) or section c(g)() and thus from the requirements of the joint final rule for non-cleared swaps and non-cleared security-based swaps pursuant to ยง _.(d)."                                                                          
[4] "for example, tripra provides that the initial and variation margin requirements of the joint final rule shall not apply to a non-cleared swap or non-cleared security-based swap in which a counterparty qualifies for an exception under section (h)()(a) of the commodity exchange act or section c(g)() of the securities exchange act, which includes certain reporting requirements established by the cftc or the sec."
[5] "however, the effect of tripra and the final rule will be to exempt many of the non-cleared swaps and non-cleared security-based swaps of these counterparties from the margin requirements of the agencies' joint final rule." 

This can be compared to the official summary:

![summary](summary.PNG)


