---
title: "Rule Summarizing CFPB"

output:
  html_document:
    code_folding: hide
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

This overviews summarizing rules using the TextRank R package, which is an application of the Google PageRank algorithm to sentences. The paper behind the package is available [here](https://web.eecs.umich.edu/~mihalcea/papers/mihalcea.emnlp04.pdf). Keywords are extracted from each sentence and a weighted graph is built estimating connected sentences. As a proof of concept, I've applied it to a (relatively) straightforward case. The rule is "Margin and Capital Requirements for Covered Swap Entities," a multi-agency final rule from 2016, available (here)[https://www.federalregister.gov/documents/2016/08/02/2016-18193/margin-and-capital-requirements-for-covered-swap-entities]. 

After tweaking the inputs iteratively, it looks like this may be an effective tool to summarize rules. A suggestion would be to apply it over single documents (such as comments) to get quick results and over sections of very long rules. Running this on a rule like the Volker Rule without reasoned subsetting would be difficult, as it constructs a Jaccard index in approximately $O(n^2)$ time where n is the number of sentences. For this example, it takes approximately 30 seconds to run from download to summary on 179 sentences.

Below is an overview of the process:

First, load a basic set of libraries, nothing too extraordinary.
```{r libs, warning=FALSE, message=FALSE}
library(tidyverse)
library(textrank)
library(xml2)
library(tidytext)
library(magrittr)
```



Then, download the rule. This could equivalently be used by loading a comment from file, but with slightly different processing.
```{r Gather Data}
link <- 'https://www.federalregister.gov/documents/full_text/xml/2020/07/22/2020-14935.xml'

xml_path <- download_xml(url = link)
xml_parse <- read_xml(xml_path) %>% xml_children() %>% xml_children() %>% xml_text()
text <- str_split(xml_parse, pattern = '\n') %>% unlist() %>% str_squish()
```

Next, we need to process the text of the rule. Steps include:
1. Remove blank strings.
2. Identify if the first character is a number to avoid including footnotes.
3. Identify headings to avoid useless information (such as "I. Background") for computational purposes.
4. Identify the text before the actual rule to avoid the summary and people's names.
5. Remove all numbers, as they trick the algorithm too much.

```{r Prep Data}
suppressWarnings(
text_df <- tibble(text = text) %>% filter(text != "") %>% mutate(id = row_number()) %>% 
                   mutate(first_char = str_sub(text, 1, 1), part = 'text') %>%  rowwise() %>% 
                   mutate(earlydot = str_detect(str_sub(text, 1, 5) ,'\\.')) %>% 
                   mutate(part = ifelse(earlydot, 'head', part)) %>% 
                   mutate(first_char = as.numeric(first_char)) %>%
                   mutate(part = ifelse(is.na(first_char), part, 'footnote')) %>% 
                   mutate(part = ifelse(id < 26, 'pre', part)) %>% 
                   ungroup() %>% mutate(text = gsub('[[:digit:]]+', '', text))
)
```
              
Once we've cleaned the text, we need to extract the sentences and words, using tidytext.    
```{r tokenize}
text_clean <- text_df %>% filter(part == 'text') %>% select(text)
text_sentences <- unnest_sentences(text_clean, output = sentences, input = text)
text_sentences <- text_sentences %>% unique() %>% mutate(textrank_id = row_number()) 
text_sentences <- text_sentences[,c(2,1)]

text_sentences %<>%
  filter(textrank_id<200)

data("stop_words")
word <- unnest_tokens(text_sentences, output = word, input = 'sentences') %>% 
  unique() %>% 
  filter(!word %in% stop_words$word)
```

This gives two tibbles.
```{r glance}
head(text_sentences)
head(word)
```

Now we can run the algorithm, which is fairly quick at this point.
```{r Run textrank}
out <- textrank::textrank_sentences(data = text_sentences, terminology = word)
```

We can finally extract a summary:
```{r,echo=FALSE}
summary(out, n = 5, keep.sentence.order =T)
```

[1] "the bureau revokes the  final rule's determination that it is an unfair practice for a lender to make covered short-term loans or covered longer-term balloon-payment loans without reasonably determining that consumers will have the ability to repay the loans according to their terms." 

[2] "however, in the preamble to the  final rule, the bureau expressed the concern that the market for these longer-term balloon-payment loans, with structures similar to payday loans that pose similar risks to consumers, might grow if only covered short-term loans were regulated under the  final rule." 

[3] "in the  nprm, the bureau did not propose to reconsider the payment provisions of the  final rule."                           

[4] "in the  final rule, the bureau found that the practice of making covered short-term or longer-term balloon-payment loans to consumers without reasonably determining if the consumers have the ability to repay them according to their terms causes or is likely to cause substantial injury to consumers."

[5] "the bureau's proposal the bureau determined in the  final rule that making covered short-term or longer-term balloon-payment loans without reasonably assessing a borrower's ability to repay the loan according to its terms is an unfair act or practice."

This can be compared to the official summary:

<!-- ![summary](summary.PNG) -->
