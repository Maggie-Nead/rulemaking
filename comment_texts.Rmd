---
title: "Tidy text analysis example: Public comments"
output: 
   html_document:
    toc: true
    code_folding: hide
---

```{r setup, include=FALSE}

# R -e "rmarkdown::render('comment_texts.Rmd')"
# git add comment_texts.html
# git add Figs
# git commit -m "update figs from linstat"

# load required packages
source("setup.R")
```

Data: 

# Convert pdf letters to text 
This function uses `pdf_text` (we can also OCR images, but I have not yet done so).
```{r, eval=FALSE}
head(list.files("comments"))

files <- tibble(file = list.files("comments")) %>% distinct()
dim(files)

files %<>% mutate(pdf = str_detect(file, "pdf"))
files %>% count(pdf)
files$file[1]

totext(files$file[1])

## FOR TESTING 
# files <- head(files)
# file_name = files$file[1]

library(pdftools)
totext <- function(file_name){
  # default value is NA
  text <- NA
  # if the file is a pdf, run pdf_text
  if(str_detect(file_name, "pdf")){
  text <- pdf_text(here("comments", file_name))  %>% 
    # collapse the list of pages 
    # FIXME 
    unlist() %>% 
    paste(collapse = "<pagebreak>") %>% 
    unlist() %>% as.character() %>% 
    paste(sep = "<pagebreak>")
  }
  return(text)
}



## If possible, read text (returns a list of char)
files$text <- map(files$file, possibly(totext, NA_real_, quiet = T))

## Inspect
files$text[1]
files %>% mutate(missing = is.na(text)) %>% count(missing)


# A helper function to clean strings
Clean_String <- function(string){
  string %<>% 
    stringr::str_replace_all("[^a-zA-Z\\s]", " ") %>% 
    stringr::str_replace_all("[\\s]+", " ") %>% 
    trimws()
  return(string)
}

# make clean and short versions of the text
files %<>% 
  mutate(text_clean = Clean_String(text)) %>% 
  mutate(text_clean_short = str_sub(text_clean, 1, 1000))

files %<>% 
  mutate(documentId = str_remove(file, "-1.pdf|-1.excel12book|-1.docx|-1.rtf|-1.msw12")) %>% 
  mutate(url = str_c("https://www.regulations.gov/document?D=", documentId))

head(files$documentId)
head(files$url)
names(files)
files$text_clean[1]
files %>% mutate(missing = text_clean == "NA") %>% count(missing)

# data that only contains file names and short text
comment_text_short <- files %>% select(documentId, text_clean_short, url)

save(comment_text_short, file = "data/comment_text_short.Rdata") # <100 MB
save(files, file = "data/comment_text.Rdata") # >100 MB


# Save data on failed downloads 
fails2 <- files %>% filter(is.na(text)) %>% select(file)
dim(fails2)
load("data/comment_fails.Rdata")
fails %<>% full_join(fails2)
save(fails, file = "data/comment_fails.Rdata") 

q()
n
git add data/comment_text_short.Rdata
git commit -m "update comment text short Rdata sample"
git pull
git push
judgelord
```


```{r comments-data, include=FALSE}
# load comments as all
load("ascending/allcomments.Rdata")
dim(all)
names(all)

d <- all
head(d$documentId)
# load(here("data/allcomments-sample.Rdata")) # for testing on a small sample of 480,000

# mass comments to the top
d %<>% arrange(desc(numberOfCommentsReceived))  %>% 
  # make a variable indicating comment is unique
  mutate(mass2 = ifelse(numberOfCommentsReceived > 99 | mass == "Mass Comments", "Mass Comments", "Other Comments"))

# format data
d$postedDate %<>% as.Date()
d$year <- as.numeric(substr(d$postedDate, 1, 4))
d$numberOfCommentsReceived %<>% as.numeric()

# docket vars 
d %<>% group_by(docketId) %>% 
  mutate(docketUnique = n()) %>% 
  mutate(docketTotal = sum(numberOfCommentsReceived)) %>% 
  ungroup() 

# year vars
d %<>% group_by(year) %>% 
  mutate(docketsPerYear = n()) %>% 
  mutate(yearTotal = sum(numberOfCommentsReceived)) %>% 
  ungroup() 

d %<>%  
  mutate(position = ifelse(grepl(" support ", commentText), "Contains \"support\"", NA )) %>% 
  mutate(position = ifelse(grepl(" oppose ", commentText), "Contains \"oppose\"", position )) %>% 
    mutate(position = ifelse(grepl(" support ", commentText) & grepl(" oppose ", commentText), "\"support\" and \"oppose\"", position )) 
```




```{r comments-per-year, fig.height=3.5, fig.width=3.5}
d %>% 
  filter(year > 2004) %>% 
  group_by(year) %>% 
  summarise(yearTotal = sum(numberOfCommentsReceived)) %>%
  ggplot() + 
  geom_col(aes(x = factor(year), y = yearTotal) ) + 
  scale_y_continuous(labels = scales::comma) + 
  labs(x = "", 
       y = paste("Total comments, N =", round(sum(d$numberOfCommentsReceived)/1000000,1), "million"),
       fill = "") + 
  theme_minimal() + 
  theme(panel.grid.major.x = element_blank(),
        panel.grid.minor.x = element_blank(),
        axis.text.x = element_text(angle = 45))
```




```{r comments-mass-vs-unique, fig.height=3.1, fig.width=5}
d %>% 
  filter(year > 2004) %>% 
  group_by(year, mass2) %>% 
  summarise(yearTotal = sum(numberOfCommentsReceived)) %>%
  ggplot() + 
  geom_col(aes(x = factor(year), y = yearTotal, fill = mass2) ) + 
  scale_y_continuous(labels = scales::comma) + 
  labs(x = "", 
       y = paste("Total Comments, N =", round(sum(d$numberOfCommentsReceived)/1000000,1), "million"),
       fill = "") + 
  theme_minimal() + 
  theme(panel.grid.major.x = element_blank(),
        panel.grid.minor.x = element_blank(),
        axis.text.x = element_text(angle = 45))
```



```{r comments-support-vs-oppose, fig.height=3.1, fig.width=5}
d %>% 
  filter(year > 2004, !is.na(position)) %>% 
  group_by(year, position) %>% 
  summarise(yearTotal = sum(numberOfCommentsReceived)) %>%
  ggplot() + 
  geom_col(aes(x = factor(year), 
               y = yearTotal, 
               fill = position) ) + 
  scale_y_continuous(labels = scales::comma) + 
  labs(x = "", 
       y = paste("Total Comments, N =", round(sum(d$numberOfCommentsReceived)/1000000,1), "million"),
       fill = "") + 
  theme_minimal() + 
  theme(panel.grid.major.x = element_blank(),
        panel.grid.minor.x = element_blank(),
        axis.text.x = element_text(angle = 45))
```

```{r comments-mass-support-vs-oppose, fig.height=3.1, fig.width=5}
d %>% 
  filter(year > 2004, !is.na(position), position != "\"support\" and \"oppose\"") %>% 
  group_by(year, position, mass) %>% 
  summarise(yearTotal = sum(numberOfCommentsReceived)) %>%
  ggplot() + 
  geom_col(aes(x = factor(year), 
               y = yearTotal, 
               fill = mass) ) + 
  scale_y_continuous(labels = scales::comma) + 
  labs(x = "", 
       y = paste("Total Comments"),
       fill = "") + 
  facet_wrap("position", strip.position="top", ncol = 1) +
  theme_minimal() + 
  theme(panel.grid.major.x = element_blank(),
        panel.grid.minor.x = element_blank(),
        axis.text.x = element_text(angle = 45))
```


```{r}
library(tidytext)

## Alternativly:
# library(textfeatures)
# textfeatures::textfeatures(text$commentText)

d$commentText %<>% str_replace_all("\n", " ")

d %<>% 
  group_by(docketId) %>% 
  mutate(n=n()) %>%
  filter(n>999) %>%
  arrange(desc(n))%>%
  arrange(desc(numberOfCommentsReceived)) %>% 
  ungroup()

# The docket with the most comments
id <- d$docketId[1]

support_oppose <- function(id){
text <- Text <- d %>% filter(docketId == id,
              !is.na(commentText),
              !str_detect(commentText, "attach|Attach"),
              nchar(commentText>15)) %>% 
  mutate(position = ifelse(is.na(position), "neither", position)) %>%   mutate(position = ifelse(position %in% c("neither", "\"support\" and \"oppose\""), "neither or both", position)) %>%
  group_by(documentId) %>%
  top_n(1, nchar(commentText)) #%>% .[1:400,]

sent <- get_sentiments("afinn") %>% 
  rbind(c("oppose", -5)) %>% 
  mutate(score = ifelse(word == "support", 5, score) ) 

s <- unnest_tokens(text,word, commentText) %>% 
  inner_join(get_sentiments("afinn")) %>% 
  group_by(documentId) %>%
  summarise(sentiment = sum(score)/n()) %>% 
  ungroup()

text %<>% 
  full_join(s) %>% arrange(sentiment)

p <- text %>% 
  drop_na(sentiment) %>%
ggplot() +
  aes(x= sentiment, 
      y= fct_reorder(documentId, sentiment),
      label = str_c(str_sub(commentText, 0, 60), "...")) + 
  geom_point(aes(color = position), alpha= .5, shape = "+", size = 6) + 
  geom_text(check_overlap = T) +
  labs(title = paste(unique(text$year), unique(text$docketTitle)),
       x = "Sentiment\n(AFINN score/word count)",
       y = "", 
       color = "")+
  theme_minimal() +
  scale_color_viridis_d() + 
  theme(axis.text = element_blank())


ggsave(filename=str_c("Figs/", "sent-", unique(Text$year), unique(Text$docketId),".png"),
       plot = p, height = 2, width = 11)
}
support_oppose(id)
walk(unique(d$docketId), safely(support_oppose))
```

```{r comments-mass, fig.height=3.1, fig.width=5}
d %>% 
  filter(year > 2004) %>% 
  group_by(year, mass) %>% 
  summarise(yearTotal = sum(numberOfCommentsReceived)) %>%
  ggplot() + 
  geom_col(aes(x = factor(year), y = yearTotal, fill = mass) ) + 
  scale_y_continuous(labels = scales::comma) + 
  labs(x = "", 
       y = paste("Total Comments, N =", round(sum(d$numberOfCommentsReceived)/1000000,1), "million"),
       fill = "") + 
  theme_minimal() + 
  scale_fill_viridis_d(begin = 0, end = .6) +
  theme(panel.grid.major.x = element_blank(),
        panel.grid.minor.x = element_blank(),
        axis.text.x = element_text(angle = 45))


``` 


```{r comments-form, fig.height=3.1, fig.width=5}
d %>% 
  filter(year > 2004) %>% 
  group_by(year, commentform) %>% 
  summarise(yearTotal = sum(numberOfCommentsReceived)) %>%
  ggplot() + 
  geom_col(aes(x = factor(year), y = yearTotal, fill = commentform) ) + 
  scale_y_continuous(labels = scales::comma) + 
  labs(x = "", 
       y = paste("Total Comments, N =", round(sum(d$numberOfCommentsReceived)/1000000,1), "million"),
       fill = "") + 
  theme_minimal() + 
  scale_fill_viridis_d(begin = 0, end = .6) +
  theme(panel.grid.major.x = element_blank(),
        panel.grid.minor.x = element_blank(),
        axis.text.x = element_text(angle = 45))
``` 



# Text reuse with a 10-gram window
```{r, eval=FALSE}
library(tokenizers)
library(tidytext)

# 10 window, fraction of 10 grams from text 1 in text 1
tengram_percent <- function(a, b){
match <- str_detect(b,
                    tokenize_ngrams(a, 
                                    n = 10, 
                                    simplify = TRUE))
return(sum(match)/length(match))
}


tengram_text <- function(a, b){
match <- str_detect(b,
                    tokenize_ngrams(a, 
                                    n = 10, 
                                    simplify = TRUE))

text <- str_c(tokenize_words(a, simplify = TRUE)[match], collapse = " ")
return(text)
}
tengram_text(text1, text2)



a <- tibble(a = c(1,2,3,4,5))

fun <- function(x){
  b <- x + a$a
}
map(a$a, fun)

tengram <- function(x){
  tokenize_ngrams(x, n = 10, simplify = TRUE)
}
str_detect_match <- function(a, b){
  match <- str_detect(a, b)
  return(sum(match)/length(match))
}

top_match <- function(text, id){
tengrams <- map(text$text, tengram)

percent <- map2_dbl(text$text, tengrams, str_detect_match)

topmatch <- tibble(id = text$id,
       percent) %>% 
  top_n(1, percent)
return(topmatch)
}




text1 <- "testing this 10 gram matching testing this 10 gram matching testing this 10 gram matching testing this 10 gram matching "
text2 <- "testing this 10 gram matching testing this 10 gram matching testing this 10 gram matching testing this 10 gram batching "
tengram(text1, text2) 

text <- tibble(text1 = text1, 
            text2 = text2,
            text = d$commentText[1:20],
            id = d$documentId[1:20])



```



